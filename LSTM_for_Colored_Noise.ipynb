{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/barauna-lo/Colored_Noise/blob/main/LSTM_for_Colored_Noise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erJ52FTN14tH"
      },
      "source": [
        "<img src=https://raw.githubusercontent.com/barauna-lo/CAP4213-Deep-Learning/main/logoinpe.png>\n",
        "\n",
        "\n",
        "# Investigating the performance of RNN (GRU and LSTM) to predict different classes of colored noise\n",
        "\n",
        "This code we gerenete Colored Noise from these python library [colorednoise](https://pypi.org/project/colorednoise/). We will also creating a datafreame for different values of $\\beta$ and creating a datafreame with theses data using pandas library. With these data, we expect to train a LSTM model for predict the signal of colored noise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\vec{F} = m \\vec{a}\n",
        "$$"
      ],
      "metadata": {
        "id": "OYIuBM0prgiE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFLwfnwu14tO"
      },
      "source": [
        "\n",
        "# Genereting / Loading the Data\n",
        "\n",
        "On the next cell we will import the librarys\n",
        "\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgOqTNud14tQ"
      },
      "source": [
        "# !pip install jupyternotify # Install de Jupyter Notify\n",
        "# %load_ext jupyternotify    # Loadgin Jupyter Notify\n",
        "!pip install colorednoise\n",
        "import colorednoise as cn\n",
        "from matplotlib import mlab\n",
        "from matplotlib import pylab as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow\n",
        "from sklearn.preprocessing import MaxAbsScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Lz0Lu6K14tY"
      },
      "source": [
        "On the abobe cell we will generate the colored noise and the power spctrum densitu using [mlab](https://matplotlib.org/stable/api/mlab_api.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7yrm11I14tZ"
      },
      "source": [
        "import colorednoise as cn\n",
        "#from matplotlib import mlab\n",
        "from matplotlib import pylab as plt\n",
        "#import numpy as np\n",
        "\n",
        "#input values\n",
        "beta = 1         # the exponent: 0=white noite; 1=pink noise;  2=red noise (also \"brownian noise\")\n",
        "samples = 2**16  # number of samples to generate (time series extension)\n",
        "\n",
        "#Deffing some colores\n",
        "beta = [0,1,2]\n",
        "colors = ['black','magenta','red']\n",
        "i = 0\n",
        "for i in range(len(beta)):\n",
        "  A = cn.powerlaw_psd_gaussian(beta[i], samples)\n",
        "\n",
        "  #Deffing the great figure size\n",
        "  plt.figure(figsize=(6,3),dpi=100)\n",
        "\n",
        "  #Ploting first subfiure\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(A, color=colors[beta[i]], linewidth=1)\n",
        "  plt.title('Colored Noise for Œ≤='+str(beta[i]))\n",
        "  plt.xlabel('Samples (time-steps)')\n",
        "  plt.ylabel('Amplitude(t)', fontsize='large')\n",
        "  plt.xlim(1,32000)\n",
        "\n",
        "  #Ploting second subfigure\n",
        "  plt.subplot(122)\n",
        "  spectrum, frequency = mlab.psd(A, NFFT=2**13)\n",
        "  plt.loglog(frequency,spectrum, color=colors[beta[i]], linewidth=0.8)\n",
        "  plt.title('Power Spectral Density of A(t) with Œ≤='+str(beta[i]))\n",
        "  plt.xlabel('Frequency')\n",
        "  plt.ylabel('Power Spectrum Density', fontsize='large')\n",
        "  plt.grid(True)\n",
        "  plt.savefig(\"color_noide_beta=\"+str(beta[i])+\".png\")\n",
        "\n",
        "#ploting the intire figure\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr34OQ5j14tb"
      },
      "source": [
        "## Generating a DataFrame Using Pandas üêº\n",
        "\n",
        "Now, we will allocate several time series in a dataframe with different colored noises for different values of $\\beta$. We will use pandas library for these Function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUjkvNaLOwmz"
      },
      "source": [
        "After generet the pandas dataframe we converting these data for the string format and save int in a `.csv` file. We must to done these becouse pandas can't write the intere values alocatad in the the cell.\n",
        "\n",
        "After that transform and save all these variables we return for the original statement for the data frame.\n",
        "\n",
        "Any time these codes runs, they will generete a new data frame and that can be a problem for comparison evaluations. For these reason I create um single data frame and hosted in kaggles on [Syntetic Color Noise](https://www.kaggle.com/luanorionbarana/synthetic-colored-noise). Also available in google drive.\n",
        "\n",
        "For these reason the next cell will be comented"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlQH30u114td"
      },
      "source": [
        "# Import pandas library üêº\n",
        "import pandas as pd\n",
        "\n",
        "#GENERATING THE COLORED NOISE DATA FRAME\n",
        "samples = 2**16\n",
        "# initialize list of lists for diferents betas\n",
        "data = []\n",
        "for i in range(0,13): #the range of beta\n",
        "    for j in range(0,10):\n",
        "        beta = round(i/4,2)                          #alocating diferents values of beta\n",
        "        A = cn.powerlaw_psd_gaussian(beta, samples)  #genereting the amplitude noise\n",
        "        s, f = mlab.psd(A, NFFT=2**13)               #genereting the power spectrum density\n",
        "        data.append([beta,A,f,s])                    #allocating the genereted data in a matrix\n",
        "\n",
        "# Create the pandas DataFrame\n",
        "df  = pd.DataFrame(data, columns = ['beta', 'Series','PSD','Frequency'])\n",
        "df1 = df\n",
        "\n",
        "#CONVERTING THE DATA FROM THE STRING FORMAT\n",
        "#df['beta'] = df['beta'].map(lambda x: ','.join(map(str, x)))\n",
        "df['Series'] = df['Series'].map(lambda x: ','.join(map(str, x)))\n",
        "df['PSD'] = df['PSD'].map(lambda x: ','.join(map(str, x)))\n",
        "df['Frequency'] = df['Frequency'].map(lambda x: ','.join(map(str, x)))\n",
        "\n",
        "#Saving the data\n",
        "df.to_csv(\"color.csv\", index=False)\n",
        "\n",
        "#Realocating the numerical values\n",
        "df = df1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2pa91cqQdBz"
      },
      "source": [
        "## Loagin a DataFrame\n",
        "\n",
        "Dowloading the standatd dataframe ```color.csv``` from the gdrive\n",
        "\n",
        "Now we will reading the data alocatad in the cvs file. Pandas can't read these values has a float. For these reason we must to convert the data for the original format we can fond whem we generat it. These code was provide for these review in [stackoverlow](https://stackoverflow.com/questions/65445581/saving-a-long-list-into-csv-in-pandas)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeLA9HWdtteR"
      },
      "source": [
        "#READING THE DATA\n",
        "\n",
        "#df = pd.read_csv(\"/home/barauna/Documentos/color.csv\")  #FOR UBUNTU\n",
        "#df = pd.read_csv(\"C:/Users/luano/Documents/color.csv\")  #FOR WINDOWS\n",
        "# !gdown --id 1V_S8hoqK9g5mCMxwRGZbsyhuK7KjxZHl            #FOR GOOGLECOLAB\n",
        "# df = pd.read_csv(\"color.csv\")\n",
        "\n",
        "#REALOCOTING THE DATA FOR THE FLOAT FORMAT\n",
        "dfn['beta'] = dfn['beta'].str.split(',').map(lambda x: list(map(float, x)))\n",
        "df['Series']    = df['Series'].str.split(',').map(   lambda x: list(map(float, x)))\n",
        "df['PSD']       = df['PSD'].str.split(',').map(      lambda x: list(map(float, x)))\n",
        "df['Frequency'] = df['Frequency'].str.split(',').map(lambda x: list(map(float, x)))\n",
        "\n",
        "\n",
        "#NORMALIZING THE DATA\n",
        "#Creating a vector with the coloumns name\n",
        "columns = df.columns[1:]\n",
        "#Loop for all the collus\n",
        "for j in range(len(columns)):\n",
        "    data=[]\n",
        "    for i in range(len(df[columns[j]])):\n",
        "        #reshaping the time series for a format that every single value inside the list is a vector,\n",
        "        #that will make a matrix with 1 column e 6k lines.\n",
        "        ghost = np.reshape(df[columns[j]][i],(len(df[columns[j]][0]),1))\n",
        "        #normalizing using `MaxAbsScaler`.\n",
        "        #That line will search for the Max VECTOR inside a list of vector and use for the\n",
        "        #normalizartion. That can correctly work for negative numbers when all the values in the\n",
        "        #\"dataframe is vectors\"\n",
        "        norm = MaxAbsScaler().fit(ghost).transform(ghost)\n",
        "        #Reshaping the normalization vector for vector in the same original format\n",
        "        #We want 1 vector with 6k data, not 1 vecto with 6k vectors\n",
        "        ghost_tensor = np.reshape(norm,(1,len(df[columns[j]][i])))\n",
        "        #alocating these value in a data vector\n",
        "        data.append(list(ghost_tensor[0]))\n",
        "    #realocating theses normalized data in the dataframe\n",
        "    df[columns[j]] = data\n",
        "    del data\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpEi0vXpFIZy"
      },
      "source": [
        "## Creating output Folders üìÇ\n",
        "\n",
        "we will set a `path` for save the files we work here. In the and we exctract all componests for a `.zip` file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4QsSAN-EUPL"
      },
      "source": [
        "!mkdir \"/content/LSTM_files/\"\n",
        "path = \"/content/LSTM_files/\"              #for googlecolab\n",
        "#path = \"C:/Users/luano/Documents/LSTM/\"     #for Windows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1EzHuIy14tg"
      },
      "source": [
        "# LSTM\n",
        "## _Long Shott Term Memory_\n",
        "Now we will introduce a Recurrent Neural Network (RNN) for these kind of RNN we will use the tensorflow library. From that library we will use the Keras for construct our LSTM model. These model was based on the these [tutorial](https://youtu.be/UbvkhuqVqUI).\n",
        "\n",
        "<img src=\"data:image/png;base64,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\" width = 300>\n",
        "\n",
        "<img src=\"data:image/png;base64,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\" width = 300>\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/31/NumPy_logo_2020.svg/2560px-NumPy_logo_2020.svg.png\" width =300>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHaLvyiPvQCk"
      },
      "source": [
        "# Prepering the Data üóíÔ∏èüèóÔ∏è\n",
        "\n",
        "Here we specially prepere the data for the LSTM arquitecture in `tensorflow`. I'ts very important to undersdand that Tensorflow work with VECTORED DATA. That's mean that every single information must to be in vector mode\n",
        "\n",
        "* Exemple\n",
        "\n",
        "numpy array format\n",
        "\n",
        "`list1 = [1,2,3,3,4,5,6,7,9]`\n",
        "\n",
        "Tensor formar\n",
        "\n",
        "`list2 = [[1],[2],[3],[4],[5],[6],[7],[8],[9]]`\n",
        "\n",
        "And we will make tese using `reshape`. See that different ways to reshape a list. We can do these using `numpy` or `tensorflow` they will return diferent type of data but aparently that dosent chang anything\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dyZqPAS14th"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Flatten"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C93xU8Y-14ti"
      },
      "source": [
        "In the next cell, we will create a function similarly to `train_test_split` from the `sklearn` library. The above logic is important because we will define how much data the LSTM model will use to predict the next step data. These functions will take the time series data and be performed by the `n_features` data training.\n",
        "\n",
        "Its import to understand that in LSTM the predicted target will be the next value of temporal series based on a quantity of pre-understood data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKgd9k-n14ti"
      },
      "source": [
        "def prepare_data(timeseries_data, n_features):\n",
        "    X, y = [],[]\n",
        "    for i in range(len(timeseries_data)):\n",
        "        # Find the end of this pattern\n",
        "        end_ix = i + n_features\n",
        "        # check if we are beyond the Sequence\n",
        "        if end_ix > len(timeseries_data)-1:\n",
        "            break\n",
        "        # gather input and output parts of the pattern\n",
        "        seq_x, seq_y = timeseries_data[i:end_ix], timeseries_data[end_ix]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return np.array(X), np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0LtHeX_14tj"
      },
      "source": [
        "### Prepering the data like sugest the boss:\n",
        "\n",
        "_pegue uma serie pequena com 50 pontos.  (pode ser um pedaco da serie para beta=2 -  qualquer uma das dez geradas).  Construa duas series testes: uma (teste1.csv)  com os primeiros 10 pontos e outra (teste2.csv) com 50 pontos. Aplique a LSTM que vc implementou sobre elas.  Comece com a de 10 pontos.  Tente prever o 11o ponto com passo 4, por exemplo.   Conta 4 e preve o 5o.  veja o exemplo do video._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3Gw2apg14tk"
      },
      "source": [
        "## We will defining a small time series data for only 10 values for $\\beta = 2$\n",
        "\n",
        "On the next cell que will chek the index of a $\\beta = 2$ temporal series"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yYaue0U14tk"
      },
      "source": [
        "df['beta'][80]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIgB1y2zYq-z"
      },
      "source": [
        "On the next cell we will will define a new `timeseries_data` and plot these time series. Here we defing a set of parameters for these time series wich is:\n",
        "\n",
        "* `serie_index` The id of the series in the dataframe.\n",
        "* `min_size` and `max_size` how define the size of the time series data used on the LSTM model for traing.\n",
        "* `n_step` defing how many numbers in the time series will be used to learn."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "timeseries_data = df['Series'][serie_index][min_size:max_size]"
      ],
      "metadata": {
        "id": "NH_WiXLB4_PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX3_RIcS14tl"
      },
      "source": [
        "# define input sequence\n",
        "serie_index = 80\n",
        "\n",
        "min_size , max_size = 0 , 1024 #Chosing a peace of the data frame\n",
        "\n",
        "timeseries_data = df['Series'][serie_index][min_size:max_size]\n",
        "\n",
        "# choose a number of time steps\n",
        "n_steps = 5\n",
        "\n",
        "# split into samples\n",
        "X, y = prepare_data(timeseries_data, n_steps)\n",
        "\n",
        "#plt.plot(df[\"Series\"][0:10])\n",
        "plt.plot(timeseries_data, color='black', linewidth=1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfcn354i14tl"
      },
      "source": [
        "On next cell will print the data that will used for traing e test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sMnh7jJ14tl"
      },
      "source": [
        "print(X),print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIxzsMz014tm"
      },
      "source": [
        "On next two cell we will plot the shape of ```X``` and _reshape_ the it. These numpy array must to have a 3 dimensional size. _(I dont know why)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRJC7Vkj14tm"
      },
      "source": [
        "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
        "n_features = 1\n",
        "X = X.reshape((X.shape[0], X.shape[1], n_features))           #Resheping with numpy\n",
        "X = tensorflow.reshape(X,(X.shape[0],X.shape[1], n_features)) #reshaping with tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vP_6g_4Y6ha7"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfibf6TI14tn"
      },
      "source": [
        "# Building The Model\n",
        "On the next cell we will construc a LSTM model compose by two hindden layers with 50 neurons with.\n",
        "\n",
        "Bild the architecture of these neural networks play a very important role in the power of prediction. Understanding how these _hyperparameters_ affect the accuracy of the model is Set\n",
        "\n",
        "* Batch Gradient Descent. Batch Size = Size of Training Set\n",
        "* Stochastic Gradient Descent. Batch Size = 1\n",
        "* Mini-Batch Gradient Descent. 1 < Batch Size < Size of Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXLsonoyfvId"
      },
      "source": [
        "### LSTM Arquitecture üèõÔ∏è\n",
        "\n",
        "On the next cell we will creat our LSTM Arquitecture.\n",
        "\n",
        "We first will fix our _seeds_ for the network dosent randomicly define the inicial width. We meke that becouse we can reconstroct the same results for the same data, and the same _hyperparameters_. To better understand (or remember rs) read [these](https://machinelearningmastery.com/reproducible-results-neural-networks-keras/) tutorial.\n",
        "\n",
        "Secondely we build de model:\n",
        "\n",
        "* ‚ö†Ô∏è Bach size = Gradiant Desendent [read more](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVLL-atK14tn"
      },
      "source": [
        "\n",
        "# define model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Flatten\n",
        "\n",
        "#Setting Random seedsüå±\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(3)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
        "model.add(LSTM(50, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "# fit model\n",
        "history = model.fit(X, y, epochs=100, verbose=1, validation_split=0.8,\n",
        "#                    batch_size= 1                   # * Stochastic Gradient Descent. Batch Size = 1\n",
        "#                    batch_size= 32 # 64 # 128       # * Mini-Batch Gradient Descent. 1 < Batch Size < Size of Training\n",
        "                    batch_size=len(timeseries_data) # * Batch Gradient Descent. Batch Size = Size of Training Set\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wzr-hj4qh824"
      },
      "source": [
        "You probably will generate several tests for several parameters, so let's save wich analyses we made it. We will generete a lot of files? Yes we will, but we can delete it before ;)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W72Yi-WHiO6S"
      },
      "source": [
        "#Alocating the results\n",
        "loss =     history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = np.arange(0,len(loss))\n",
        "\n",
        "#Defing a general file name\n",
        "filename_traingXloss =\"TraingLoss_beta\"+str(df['beta'][serie_index])+\"_timeSerieSize=\"+str(max_size-min_size)+\"_nsteps=\"+str(n_steps)+\"_eposch=\"+str(len(epochs))\n",
        "\n",
        "#Saving a DataFrame with the Results\n",
        "df_results = pd.DataFrame({'loss': loss, 'val_los': val_loss,'epochs':epochs})\n",
        "df_results.to_csv((filename_traingXloss+\".csv\"), index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzBgEcpfAa-X"
      },
      "source": [
        "Let's plot the chat of traing and validation Accuray and Loss of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaoY-vd4AG3H"
      },
      "source": [
        "#Plot the Training and Validation loss\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(epochs,loss,    label='Training loss',  color='blue')\n",
        "plt.plot(epochs,val_loss,label='Validation loss',color='red')\n",
        "# Ploting the max value of vall_loss. We dont need these all the time, only whem test limits of the network\n",
        "#plt.scatter(history.history['val_loss'].index(max(history.history['val_loss'])),max(history.history['val_loss']),label=('Max Val Loss ='+str(round(max(history.history['val_loss']),2))+\"\\nEpoch = \"+str(history.history['val_loss'].index(max(history.history['val_loss'])))),color='purple',zorder=3,marker='*',linewidths=2)\n",
        "plt.legend()\n",
        "plt.title(\"Training and Validation loss for $n_{steps} =$\"+str(n_steps))\n",
        "# time = str(datetime.now().strftime('%Y-%m-%d %H:%M:%S')).replace(\":\",\"_\") # What time is that?\n",
        "plt.savefig(filename_traingXloss+\".pdf\")                             #Saving the figure with the currently data and time\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIy1B07H14to"
      },
      "source": [
        "On the next cell we will again redefing some parameters such as\n",
        "\n",
        "* ```inti``` how will fit the value of witch part of the data frame we will select to test\n",
        "* ```x_input``` will be a stat point fron the in the meadle (or not) data frame data having ```n_steps``` size\n",
        "* ```i``` is the inicial value for initial step ¬ø\n",
        "* ```f``` the value of the number of predicted values\n",
        "\n",
        "In this cell, the RNN will start to predict the new steps of the time series called _days_. They will be running from an `input` vector from the size of  `max_size`$-$ `range` of the series on the data frame. It's important to observe that the initial points here are a group of points they already have learned. If `days` + `range` < `max_size`, the RNN will construct a learned prediction.\n",
        "\n",
        "### ‚ö†Ô∏è ATENTION ‚ö†Ô∏è\n",
        "\n",
        "The ```#```s in the ```while``` can be uncomented for print the step by step results\n",
        "\n",
        "In the end, we will creatin a vector called `day_new` how will have the same length of the `time_series` and a vector `day_pred` which will have the size of the predicted data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0J5H8TG14tp"
      },
      "source": [
        "# demonstrate prediction for next `days` days\n",
        "days = 20\n",
        "range = 500\n",
        "#n_steps =10\n",
        "\n",
        "init = max_size - range\n",
        "\n",
        "x_input = np.array(  df['Series'][serie_index][init:init + n_steps] ) # he is just working for n_stpes +1\n",
        "temp_input=list(x_input)\n",
        "\n",
        "lst_output=[]\n",
        "i=0\n",
        "f = days #number of newsteps\n",
        "while(i<f):\n",
        "# N√£o entendi muito bem o que ele fez aqui!\n",
        "    if(len(temp_input)>n_steps): # !!!! ON THE ORIGNINAL CODE THAT IS JUST 3\n",
        "        x_input=np.array(temp_input[1:])\n",
        "#        print(\"{} day input {}\".format(i,x_input))\n",
        "        #print(x_input)\n",
        "        x_input = x_input.reshape((1, n_steps, n_features))\n",
        "        #print(x_input)\n",
        "        yhat = model.predict(x_input, verbose=0)\n",
        "#        print(\"{} day output {}\".format(i,yhat))\n",
        "        temp_input.append(yhat[0][0])\n",
        "        temp_input=temp_input[1:]\n",
        "        #print(temp_input)\n",
        "        lst_output.append(yhat[0][0])\n",
        "        i=i+1\n",
        "    else:\n",
        "        x_input = x_input.reshape((1, n_steps, n_features))\n",
        "        yhat = model.predict(x_input, verbose=0)\n",
        "#        print(yhat[0])\n",
        "        temp_input.append(yhat[0][0])\n",
        "        lst_output.append(yhat[0][0])\n",
        "        i=i+1\n",
        "#print(lst_output)\n",
        "\n",
        "day_new=np.arange(min_size , max_size)\n",
        "day_pred=np.arange(init,init+len(lst_output))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CPiaGfxjzyH"
      },
      "source": [
        "For the last, we will plot a chart with the orignal data from the time series, the start point from the learned model and the expected data not learnded on the time series\n",
        "\n",
        "_Ta certo isso mesmo? Conferir viu?_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcLkVTFbDKtU"
      },
      "source": [
        "from  datetime import datetime\n",
        "plt.figure(figsize=(10, 4))\n",
        "#plt.subplot(1,2,1)\n",
        "#Original synsteic data\n",
        "plt.plot(day_new[:init+1],timeseries_data[:init+1],label='Time Series',color=\"black\",linewidth=0.8)\n",
        "#Next expected data\n",
        "plt.plot(day_pred,df['Series'][serie_index][init:init+len(lst_output)],color='gray',linewidth=0.5,label='Expected Data',linestyle='--')\n",
        "#Predicted Model\n",
        "plt.plot(day_pred,lst_output,label='Predict Model')\n",
        "#Limists\n",
        "plt.xlim(0 if init-len(lst_output)< 0 else init-len(lst_output),init+len(lst_output))\n",
        "#Ploting the lerning limit in the data series\n",
        "if day_pred[-1] > max_size and  1:\n",
        "    plt.axvline(x=max_size,linestyle='--',color='red',linewidth=1,label='Learned Limet')\n",
        "else: pass\n",
        "#Ploting the outliers for the timeseries_data\n",
        "plt.axhline(y= 3*np.std(timeseries_data), linestyle='--',color='blue',linewidth=1,label='Outlier')\n",
        "plt.axhline(y=-3*np.std(timeseries_data), linestyle='--',color='blue',linewidth=1)\n",
        "plt.legend()\n",
        "#File Name\n",
        "filename_model_results = \"MODEL_beta\"+str(df['beta'][serie_index])+\"_timeSerieSize=\"+str(max_size-min_size)+\"_nsteps=\"+str(n_steps)+\"_days=\"+str(days)+\"_eposch=\"+str(len(epochs))+\"bath_size\"+str(len(timeseries_data))\n",
        "#Alocating the time model genetrion\n",
        "time = str(datetime.now().strftime('%Y-%m-%d %H:%M:%S')).replace(\":\",\"_\")\n",
        "plt.savefig(time+filename_model_results+\".pdf\")\n",
        "#plt.show()/content/sample_data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbKW6rf692Ud"
      },
      "source": [
        "# Rubens's Code\n",
        "\n",
        "<img src=\"https://web.whatsapp.com/pp?e=https%3A%2F%2Fpps.whatsapp.net%2Fv%2Ft61.24694-24%2F55859438_571733520003712_8198767391795052544_n.jpg%3Fccb%3D11-4%26oh%3Dcb9061e7962a968cf9b86b9b5fcb6c1b%26oe%3D6190346C&t=l&u=5512991991800%40c.us&i=1404055322&n=EoVUuydTKeWBkGkSQA2Gm2sSBZtHvgc3g7cJJXbbxwQ%3D\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vi0q2kQ2WVJ"
      },
      "source": [
        "from numpy.random import normal\n",
        "import numpy as np\n",
        "\n",
        "#1 -> 1D\n",
        "#1.45 -> 2D\n",
        "#\n",
        "\n",
        "def cNoise(beta,shape=(1024,),std=0.001):\n",
        "    '''\n",
        "       Wrote by: Rubens Andreas Sautter (2021)\n",
        "\n",
        "       Approximating spectral decay by the squared root of the 1/(f^n).\n",
        "       Frequency are measured in multidimensional space by the frequency euclidian distance\n",
        "\n",
        "       The gaussian standard deviation\n",
        "\n",
        "       Based on paper:\n",
        "      http://articles.adsabs.harvard.edu//full/1995A%26A...300..707T/0000707.000.html\n",
        "    '''\n",
        "    dimension = []\n",
        "    for index,dsize in enumerate(shape):\n",
        "        dimension.append(np.fft.fftfreq(dsize).tolist())\n",
        "    dimension = tuple(dimension)\n",
        "    d = float(len(dimension))\n",
        "\n",
        "    freqs = np.power(np.sum(np.array(np.meshgrid(*dimension,indexing='ij'))**2,axis=0),1/2)\n",
        "\n",
        "    #Sampling gaussian with sandard deviation varying according to frequency\n",
        "    ftSample = normal(loc=0,scale=std,size=shape) + 1j*normal(loc=0,scale=std,size=shape)\n",
        "\n",
        "    # Setting the scale [0,2pi]\n",
        "    freqs = np.pi*freqs\n",
        "    not0Freq = (np.abs(freqs)>1e-15)\n",
        "\n",
        "    decayCorrection = np.sqrt(2)**(d-1)\n",
        "\n",
        "    scaling = (freqs[not0Freq]+0j)**(-(beta*decayCorrection )/2)\n",
        "\n",
        "    ftSample[not0Freq] = (ftSample[not0Freq]*scaling)\n",
        "\n",
        "    not0Freq = (np.abs(freqs)>1e-15)\n",
        "\n",
        "    spsd = np.sum(np.abs(ftSample))\n",
        "\n",
        "    # zero avg\n",
        "    ftSample[0] = 0.0\n",
        "\n",
        "    out = np.fft.ifftn(ftSample*spsd).real\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SbDt5bl5H4e"
      },
      "source": [
        "A = cNoise(beta,shape=(1024,),std=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH6vlS6q5LBu"
      },
      "source": [
        "#input values\n",
        "beta = 1         # the exponent: 0=white noite; 1=pink noise;  2=red noise (also \"brownian noise\")\n",
        "samples = 2**16 # number of samples to generate (time series extension)\n",
        "#beta = [0,1,2]\n",
        "\n",
        "#Deffing somo colores\n",
        "colors = ['black','pink','brown']\n",
        "\n",
        "A = cNoise(beta,shape=(1024,),std=0.001)\n",
        "A0 = cn.powerlaw_psd_gaussian(beta, 1024)\n",
        "#A = np.append(beta,A)\n",
        "\n",
        "#Deffing the great figure size\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "#Ploting first subfiure\n",
        "plt.subplot(1,2,1)\n",
        "# plt.plot(A, color=colors[beta], linewidth=1)\n",
        "# plt.title('Colored Noise for Œ≤='+str(beta))\n",
        "# plt.xlabel('Samples (time-steps)')\n",
        "# plt.ylabel('Amplitude(t)', fontsize='large')\n",
        "spectrum, frequency = mlab.psd(A, NFFT=2**13)\n",
        "plt.loglog(frequency,spectrum, color=colors[beta], linewidth=0.8)\n",
        "plt.title('Power Spectral Density of A(t) with Œ≤='+str(beta))\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Power Spectrum Density', fontsize='large')\n",
        "plt.grid(True)\n",
        "plt.savefig(\"color_noide_beta=\"+str(beta)+\".pdf\")\n",
        "\n",
        "\n",
        "\n",
        "#Ploting second subfigure\n",
        "plt.subplot(122)\n",
        "# plt.plot(A0, color=colors[beta], linewidth=1)\n",
        "# plt.title('Colored Noise for Œ≤='+str(beta))\n",
        "# plt.xlabel('Samples (time-steps)')\n",
        "# plt.ylabel('Amplitude(t)', fontsize='large')\n",
        "# #plt.savefig(\"color_noide_beta=\"+str(beta)+\".pdf\")\n",
        "spectrum, frequency = mlab.psd(A0, NFFT=2**13)\n",
        "plt.loglog(frequency,spectrum, color=colors[beta], linewidth=0.8)\n",
        "plt.title('Power Spectral Density of A(t) with Œ≤='+str(beta))\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Power Spectrum Density', fontsize='large')\n",
        "plt.grid(True)\n",
        "plt.savefig(\"color_noide_beta=\"+str(beta)+\".pdf\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#ploting the intire figure\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0HMCKiRLJSa"
      },
      "source": [
        "\n",
        "# Zip File üì¶\n",
        "\n",
        "Creating a zip file with all the files stored here for future analyses.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O2jQ-dq5wF0"
      },
      "source": [
        "import os\n",
        "from datetime import datetime #you have probably already import that\n",
        "from zipfile import ZipFile\n",
        "from os.path import basename\n",
        "date = datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "# create a ZipFile object\n",
        "with ZipFile(date+'_LSTM_files.zip', 'w') as zipObj:\n",
        "   # Iterate over all the files in directory\n",
        "   for folderName, subfolders, filenames in os.walk(path):\n",
        "       for filename in filenames:\n",
        "           #create complete filepath of file in directory\n",
        "           filePath = os.path.join(folderName, filename)\n",
        "           # Add file to zip\n",
        "           zipObj.write(filePath, basename(filePath))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxrfQNHFNacX"
      },
      "source": [
        "#Ziping Data for downloadüì¶\n",
        "!zip -r /content/All_Files.zip /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cdJZ9GVUMwVm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}